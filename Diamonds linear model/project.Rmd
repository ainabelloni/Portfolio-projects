---
title: "Diamonds - Final Assignment"
author: "Aina Belloni"
date: "16/3/2021"
output: pdf_document
---
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
# Diamonds dataset

Searching on the Internet I found this dataset called `Diamonds` in the website [Kaggle.com](www.kaggle.com). I decided to take into consideration the information related to some diamonds contained in this file because I think it's interesting see how some variables that describe a diamond  can influentiate the determination of the price of that diamond.

The dataset downloaded from the website was very big, I decided to reduce from the beginning the dimension of that from 53940 observations to 10000. 

I also transformed all the variables containing numbers in numeric variables with the function `as.numeric` and the qualitative/categorical variables as factor variables using the function `as.factor`. 

At the end I remove the first variable which was the ID number of the diamonds which simply matched the row number of the dataset. 

The goal of this project is to understand what kind of data we are dealing with, determinate the best linear model that allows us to predict the price of any diamond and understand the problems that we can encounter during the analysis.

## Load the data
```{r}
setwd("~/CATTOLICA/Applied linear models/assignments")
diamonds<-read.csv2("diamonds.csv", header=T, sep=",",dec=".")
```

```{r include=FALSE}
set.seed(150798)
new=sample(1:53940,10000)

diamonds=diamonds[new,-1]

diamonds$carat=as.numeric(diamonds$carat)
diamonds$cut=as.factor(diamonds$cut)
diamonds$color=as.factor(diamonds$color)
diamonds$clarity=as.factor(diamonds$clarity)
diamonds$depth=as.numeric(diamonds$depth)
diamonds$table=as.numeric(diamonds$table)
diamonds$price=as.numeric(diamonds$price)
diamonds$x=as.numeric(diamonds$x)
diamonds$y=as.numeric(diamonds$y)
diamonds$z=as.numeric(diamonds$z)

rownames(diamonds)=1:nrow(diamonds)

library(ggplot2)
library(viridis)
library(RColorBrewer)
```

```{r}
str(diamonds)
```
\
The dataset now includes information about 10000 diamonds, in particular we have 10000 rows/observations and 10 variables (columns). The variables are :

* `carat` : the number of carats, so the weight of the diamond
* `cut` : the quality of the cut (Fair, Good, Very Good, Premium, Ideal) where Fair is the worst and Ideal the best
* `color` : the color of the diamond (D,E,F,G,H,I,J) where D is the best and J the worst
* `clarity` : a measurement of how clear the diamond is (I1, SI2, SI1, VS2, VS1, VVS2, VVS1, IF) where I1 is the worst and IF the best.
* `depth` : total depth percentage
* `table` : width of top of diamond relative to the widest point
* `x`,`y` and `z` : the 3 dimensions of the diamond respectively length, width and depth

## Exploratory analysis 

Since the variables `cut`, `color` and `clarity` are categorical, I exclude them from the scatterplot matrix.

```{r fig.height=7, fig.width=10}
plot(diamonds[,-c(2,3,4)], col="forestgreen", main="Scatterplot data diamonds")  
```
We see a strong linear correlation between the variables `x`,`y` and `z`, and also these variables and the `carat` and `price` ones. Another quite important relationship is between `price` and `carat` variables. For the others the relationship is unclear.

To deepen the analysis we can also see the correlation matrix.
\
```{r fig.height=5, fig.width=10, message=FALSE, warning=FALSE}
corr=cor(diamonds[,-c(2,3,4)]) 

library("corrplot")
corrplot(corr,method="color", addCoef.col="black", tl.col="black",tl.srt=45, 
         col = brewer.pal(n = 8, name = "PRGn"))
```
\
There are many variables strongly correlated.

### Qualitative variables in relation with the price variable

```{r fig.height=5, fig.width=10}
par(mfrow=c(1,2),mar=c(3,3,3,.5), mgp=c(1.7,.5,0))
boxplot(price~cut, data=diamonds, col="forestgreen", main="Price ~ Cut")
boxplot(price~color, data=diamonds, col="forestgreen", main="Price ~ Color")
par(mfrow=c(1,1),mar=c(3,4,2,4))
boxplot(price~clarity, data=diamonds, col="forestgreen", main="Price ~ Clarity")
```

# Best subset selection of the variables

Now we consider a model whose response is the variable `price` and regressors are all the other variables in the dataset. We try also to add an interaction between the weight of the diamond (`carat`) and the length of it (`x`). 

Using the `regsubsets()` function we perform a Best Subset Selection, whose number of models considered would be $2^p=2^{24}=16777216$. 

```{r}
library(leaps)
ols_bss <- regsubsets(price ~ .+(carat*x),data=diamonds, nvmax=24)
summ_bss <- summary(ols_bss)
```

Now that the function provides us the best $24$ models, each one containing respectively $1,2,3,...,24$ predictors, we have to choose the best among those, using different criteria.

* BIC : Bayesian information criteria 
* Mallow's Cp 
* Adjusted $R^2$
* Cross-Validation error
\
```{r fig.height=8, fig.width=10}
par(mfrow=c(2,2))

plot(summ_bss$bic, type="b", pch=19,
xlab="Number of predictors", ylab="", main="Drop in BIC", col="forestgreen")
abline (v=which.min(summ_bss$bic),col = "magenta4", lty=2,lwd=1.5)

plot(summ_bss$adjr2, type="b", pch=19,
xlab="Number of predictors", ylab="", main="Adjusted Rˆ2",col="forestgreen")
abline (v=which.max(summ_bss$adjr2),col = "magenta4", lty=2)

plot(summ_bss$cp, type="b", pch=19,
xlab="Number of predictors", ylab="", main="Mallow’ Cp",col="forestgreen")
abline (v=which.min(summ_bss$cp),col = "magenta4", lty=2)

p <- 24
k <- 10
set.seed (1)
folds <- sample (1:k,nrow(diamonds),replace =TRUE)
cv.errors <- matrix (NA ,k, p, dimnames =list(NULL , paste (1:p) ))
for(j in 1:k){
  best.fit =regsubsets (price ~ .+(carat*x), data=diamonds[folds!=j,], nvmax=24)
  for(i in 1:p) {
    mat <- model.matrix(as.formula(best.fit$call[[2]]),diamonds[folds==j,])
    coefi <- coef(best.fit ,id = i)
    xvars <- names(coefi )
    pred <- mat[,xvars ]%*% coefi
    cv.errors[j,i] <- mean( (diamonds$price[folds==j] - pred)^2)
  }
}
cv.mean <- colMeans(cv.errors)

plot(cv.mean ,type="b",pch=19, xlab="Number of predictors",
     ylab="CV error", col="forestgreen", main="CV error")
abline(v=which.min(cv.mean), col="magenta4", lty=2)
```
\
The number of predictors for the best model chosen, according to the following criteria.

```{r}
param <- c(which.min(summ_bss$bic),which.max(summ_bss$adjr2),
           which.min(summ_bss$cp),which.min(cv.mean))
names(param) <- c("BIC","Adj Rˆ2", "Cp","CV-error")
param
```
We can see that the first 3 quantities give the same result, while the CV error is at his minimum for a very low number of parameters.

Looking at the summary of the `regsubset()` function I decided to take into consideration the model with 20 parameters, and the interaction is not included.

```{r}
coef(ols_bss,20)
```

# Collinearity issues

Now that we have our chosen model, we have to verify the presence of some problems, for example the collinearity.

```{r message=FALSE, warning=FALSE}
ols <- lm(price ~ .-table-y-z, data=diamonds)

library(car)
vif(ols)[,1]
```
We have collinearity for variable `carat` and `x`, I expected it since from the correlation matrix we have seen a high correlation between the two.

Since we have another variable of measure of the diamond which is the depth, we try to exclude the variable `x` from the model.

```{r}
ols <- update(ols, ~ .-x, data=diamonds)
summ <- summary(ols)

vif(ols)[,1]
```
In fact, the collinearity is no longer an issue of our model.

```{r}
n <- nrow(diamonds)
p <- nrow(summ$coefficients)-1
```


# Diagnostic

It's interesting to observe some different plots, in order to individuate if the model doesn't satisfy the assumptions made for the Multiple Linear Regression model.

First of all we plot the fitted values $\hat{y}$ versus the residuals.

```{r include=F}
par(mfrow =c(1,1), mar=c(4,4,2,4))
```

```{r fig.height=6, fig.width=8}
plot(ols, which=1, lwd=2, col="forestgreen")

```

## Variance of the errors

There is a strong evidence of non-constant variance of the errors, so there is heteroschedasticity.

## Linearity of the model

The fitted model line seems to have a slight paraboid shape, so we could say that the relationship between the response and the predictors is not very linear.

\
\
\
\
\

## Normality assumption of the errors

To verify this assumption we have to provide the QQ-PLOT with the respected QQ-line and the result of the Shapiro-Wilk test.

```{r fig.height=5, fig.width=8}
plot(ols, which=2, col="forestgreen")
qqline(rstandard(ols), col=2,lwd=2)
```
\
Since the function of the Shapiro-Wilk test works if the number of observations is less or equal to 5000, I sample a smaller dataset.

```{r}
newdiamonds <- diamonds[sample(nrow(diamonds),5000),]
ols_new <- lm(price ~ carat+cut+color+clarity+depth, data=newdiamonds)
shapiro.test(residuals(ols_new))
```

From the qq-plot and the test we can say that the hypothesis of normality of the model has to be rejected. Observing the plot we could say that we have a long tailed distribution, in this case we should change the inference on another distribution.

\
\
\

## Large leverage points

We want now to individuate points with high leverage, so we observe the plot of the leverages in relation with the standardized residuals.

```{r fig.height=6, fig.width=8}
plot(ols, which=5, lwd=2, col="forestgreen")
```
```{r}
hii <- influence(ols)$hat
n <- nrow(diamonds)
p <- nrow(summ$coefficients)-1
high_leverage=hii[which(hii>=(2*(p+1)/n))]

length(high_leverage)
```
It seems that the data contain 528 observations with high leverage, from the plot we can individuate in particular 3 points with very high leverage.

In particular, these observations we are discussing are the observations with row number :

```{r}
names(sort(high_leverage, decreasing = T)[1:3])
```
\

## Outliers

From the first plot, the residual plot, we can observe that there are 5 observations very far from the others, 3 of them are observation number 50, 3730 and 2428.

According to the rule of thumb, which says that an observation is a possible outlier if the standardize residual is $>|3|$, we have that possible outliers in our data are :

```{r}
rsta <- rstandard(ols)
length(rsta[which(rsta>abs(3))])
```
## Influential points

In order to combine the information above on outliers and high leverage points we use a measure called Cook's distance that allow us to see which are the points that most influence the model.

```{r fig.height=5, fig.width=7}
plot(ols, which=6, lwd=2, col="forestgreen")

```
From the plot we see that there are at least 3 points that are far from the others, we calculate them.

```{r}
cook <- cooks.distance(ols)
infl_points <- sort(cook,decreasing=T)[1:3]
infl_points
```

# Improve the model

* For the non-constant variance we should transform the response, in particular a logarithmic transformation could be useful.
* For the non linearity of the model, we should verify which of the predictor has the most non linear relationships with the residuals, and in our case is the variable `carat`, and a logarithmic transformation also of this variable could improve the model.
* For the non normality of the errors, we ignore it for now.
* Since we have a very big dataset and only a few influential points, which don't exceed the threshold, I think it's not necessary to remove them from the fitting.

The new model will be the following one :
```{r}
ols <- lm(log(price) ~ log(carat)+ cut + color + clarity + depth, data=diamonds)
summ <- summary(ols)

plot(ols, which=1, lwd=2, col="forestgreen")

round(summ$coefficients,4)
```

## Interpretation parameters and uncertainties

The fitted model has the following structure :

$$\hat{Y}=\hat{\beta}_0+\hat{\beta}_1X_1+\hat{\beta}_2X_2+\hat{\beta}_3X_3+...+\hat{\beta}_{19}X_{19}$$
Where:

* $\hat{\beta}_0$ is the intercept and is estimated to be 7.922. It corresponds to the expected logarithm of the price of the diamond in a state with no carats, color D, fair cut, clarity I1, no depth and no table. 

* $\hat{\beta}_1$ is the coefficient for the logarithm of the number of carats (`log(carat)`) whose estimation is 1.883 which means that if the logarithm of the carats increase by 1 the log price rises by 1.883, keeping the other variables constant.

* $\hat{\beta}_2,\hat{\beta}_3,\hat{\beta}_4,\hat{\beta}_5$ are coefficients for the variable dummy `cut` which assumes 1 if it is the cut that corresponds to the diamond, 0 otherwise. If each of them are 0 the cut of the diamond is the "Fair" one, which is the baseline. The coefficients of those variables are all positive, it means that if the cut is not Fair (which is the worst cut) the logarithm of the price,keeping the other variables constant, increases respectively by 0.086, 0.165, 0.146 and 0.124 for cut Good, Ideal, Premium and Very Good. 

* $\hat{\beta}_6,\hat{\beta}_7,\hat{\beta}_8,\hat{\beta}_9,\hat{\beta}_{10},\hat{\beta}_{11}$ are coefficients for the dummy variable `color` which assumes 1 if it is the color that corresponds to the one of the diamond (E,F,G,H,I,J), 0 otherwise. If each of them are 0 the color is the color D, which is the baseline. The coefficients of those variables are all negative, it means that if the color is not color D (which is the best) the logarithm of the price, keeping the other variables constant, decreases respectively by 0.049, 0.093, 0.154, 0.246, 0.367 and 0.512 for color E,F,G,H,I and J. 

* $\hat{\beta}_{12},\hat{\beta}_{13},\hat{\beta}_{14},\hat{\beta}_{15},\hat{\beta}_{16},\hat{\beta}_{17},\hat{\beta}_{18}$ are variables for the dummy `clarity` which assumes 1 if it is the clarity that corresponds to the one of the diamond (IF,SI1,SI2,VS1,VS2,VVS1,VVS2), 0 otherwise. If each of them are 0 the clarity is 'I1', which is the baseline. The coefficients of those variables are all positive, it means that if the clarity is not I1 (which is the worst) the log price, keeping the other variables constant, increases respectively by 1.086, 0.573, 0.415, 0.792, 0.729, 1.01 and 0.931 for clarity IF,SI1,SI2,VS1,VS2,VVS1 and VVS2. 

* $\hat{\beta}_{19}$ is the coefficient for the variable `depth` , with all the other regressors in the model held fixed, increasing depth of a diamond by 1, decreases the logarithm of the price in average by 0.001, keeping the other variables constant. 

\
The second column provide the estimated standard deviation for each coefficient and it's a measure that tells us the range in which probably would stay the coefficient. 

So for example for the `depth` variable the value of $\hat{\beta}_{19}$ mostly stands in the range $-0.0009\pm{0.0010}$ which is a very very high standard error. The same interpretation for each of the other coefficients, but for the others the standard error in proportion of the coefficient is a lower value.
\
```{r warning=F, message=F,fig.height=5, fig.width=8}
library(effects)
plot(predictorEffect("depth",ols,main=""),lines=list(col="forestgreen"))
```

### Estimated standard error

```{r}
summ$sigma
```
It measures the error of the regression line’s predictability, $\hat{\sigma}=0.135$. It means that the errors for the logarithm of the price are mostly in the range $\pm{0.135}$ 

### Coefficient of determination R^2

```{r}
summ$r.squared
```

So $R^2=0.98$ and it's a very high value, it means that the regressors explain the 98% of the variation of the log(price).

## Significance test on coefficients

In the summary of the model above, for each coefficient is indicated the T-test and the pvalue associated. The T-test is the statistical test for the test of the following hypothesis :

$$H_0:\beta_j=0\longrightarrow{T_{test}=\hat{\beta}_j/se(\hat{\beta}_j)}$$
For all the coefficients except for the one associated to the `depth` variable, we can conclude that the p-value is very close to zero, so the null hypothesis is rejected. While $\hat{\beta}_{19}$ is non significant because the associated p-value is > 5%.

In fact, we can provide the 95% confidence interval for each coefficient :
```{r}
confint.lm(ols)
```

Only the variable `depth` contain the value 0 in the interval.

## Test of a group of regressors

Since in the best subset selection the Cross Validation error was at his minimum for the model with only 15 variables, and this model wouldn't have included the `cut` variable, it's interesting to test the regressors for the dummy variable `cut`.

$$H_0:\beta_2=\beta_3=\beta_4=\beta_5=0$$

```{r}
ols1 <- lm(log(price) ~ log(carat)+color+clarity+depth,data=diamonds)
anova(ols1,ols)
```

The pvalue of the F test for those regressors is very low, so we have to reject the null hypothesis and we should remain with the original model.

## Test all the regressors

$$H_0:\beta_1=\beta_2=...=\beta_{18}=\beta_{19}=0$$

```{r}
summ$fstatistic
```
\
```{r}
1-pf(summ$fstatistic[1],summ$fstatistic[2],summ$fstatistic[3])
```

Pvalue is very low, tend to 0, so we have to reject the null hypothesis, and the model is significant.

# Prediction

We suppose to have a new information about a 1.5 carat diamond, good cut, with a H color and clarity type IF, whose depth is 60. 
\

```{r}
newdata = data.frame(carat=1.5, cut="Good",color="H",clarity="IF", depth=60)
predict(ols, newdata = newdata, interval="prediction",level=.95)
```

The prediction of the logarithm of the price is 9.55, and with a 95% of probability the real value would stay between 9.29 and 9.82.

# Simulation of a new dataset

Given the estimated parameters fitted in the model above, we sample 100 data from the design matrix and we calculate the fitted values.

```{r}
set.seed(1)
n <- 100
beta <- ols$coefficients ; sigma <- summary(ols)$sigma
X <- model.matrix(ols)
obs_simu <- sample(1:nrow(X),n)
new_X <- X[obs_simu,]

y_hat=new_X%*%beta+(rep(1,n)*rnorm(n, mean=0, sd=sigma))
fake_data <- data.frame(new_X[,-1],exp(y_hat))
fake_data[,1] <- exp(fake_data[,1])
colnames(fake_data)[c(1,20)] <- c("carat","price")
```
```{r fig.height=4, fig.width=8}
plot(fake_data$price, diamonds$price[obs_simu], xlab="Simulated price", ylab="Observed price", 
     main="Observed data vs simulated data", col="forestgreen")
abline(0,1, col=2, lwd=2)
```

We used the fitting of the improved model to create a new dataset containing the new prices calculated on the base of the estimated parameters and taking into account also the normal distributed error for each observation. From the plot we should say that simulated and observed data are very similar, more for low prices than high prices. 

# Conclusions

What we can conclude from this project is that the Multiple Linear Model is not a perfect model for our data, but a good one, we had many issues and many assumptions were incorrect, but with some transformations and the good variables selected we obtain such a quite good model.

In particular variables containing information on the number of carats, the cut, the color, the clarity and the depth are very useful to predict the price of a diamond, those variables explain a very high percentage of the variability of the price for the multiple linear model.






